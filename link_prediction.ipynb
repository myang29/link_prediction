{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in training set and build neighbour dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"train.txt\",\"r\")\n",
    "raw_data = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4867136\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "node_set = set()\n",
    "in_neighbour = defaultdict(set)  # a node is followed by another nodes {key:node, values:neighbour(in set)}\n",
    "out_neighbour = defaultdict(set) # a node follow another nodes, format {key:node, values:neighbour(in set)}\n",
    "for instance in raw_data:  # change to full dataset\n",
    "    source,sink_list = instance.split()[0],instance.split()[1:]\n",
    "    node_set.add(source)\n",
    "    out_neighbour[source] = set(sink_list)\n",
    "    for node in sink_list:\n",
    "        node_set.add(node)\n",
    "        in_neighbour[node].add(source)\n",
    "#print (in_neighbour)\n",
    "print (len(node_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def common_neighbour(node1,node2):\n",
    "    return len(in_neighbour[node1].intersection(in_neighbour[node2]))+len(out_neighbour[node1].intersection(out_neighbour[node2]))\n",
    "\n",
    "def salton(node1,node2):\n",
    "    return common_neighbour(node1,node2)/math.sqrt((len(in_neighbour[node1])+len(out_neighbour[node1]))*(len(in_neighbour[node2])+len(out_neighbour[node2])))\n",
    "\n",
    "def Jaccard(node1,node2):\n",
    "    return common_neighbour(node1,node2)/(len(in_neighbour[node1].union(in_neighbour[node2]))+len(out_neighbour[node1].union(out_neighbour[node2])))\n",
    "\n",
    "def AA(node1,node2):\n",
    "    result = 0\n",
    "    intersection = in_neighbour[node1].intersection(in_neighbour[node2]).union(out_neighbour[node1].intersection(out_neighbour[node2]))\n",
    "    for node in intersection:\n",
    "        result += (1/math.log(len(in_neighbour[node])+len(out_neighbour[node])))\n",
    "    return result\n",
    "\n",
    "def allocation(node1,node2):\n",
    "    result = 0\n",
    "    common1 = in_neighbour[node1].intersection(in_neighbour[node2]).union(out_neighbour[node1].intersection(out_neighbour[node2]))\n",
    "    common2 = in_neighbour[node1].intersection(out_neighbour[node2]).union(out_neighbour[node1].intersection(in_neighbour[node2]))\n",
    "    common = common1.union(common2)\n",
    "    \n",
    "    #common = common = in_neighbour[node1].intersection(out_neighbour[node2]).union(out_neighbour[node1].intersection(in_neighbour[node2]))\n",
    "    \n",
    "    for node in common:\n",
    "        result += 1/(len(out_neighbour[node])+len(in_neighbour[node]))\n",
    "    return result\n",
    "\n",
    "\n",
    "def knn(node1,node2):\n",
    "    result=[]\n",
    "    firts=1\n",
    "    Wbin=1/math.sqrt(1+len(in_neighbour[node2]))\n",
    "    Waout=1/math.sqrt(1+len(out_neighbour[node1]))\n",
    "    #w2 = Waout\n",
    "    #w3 = Wbin\n",
    "    w4=Wbin+Waout\n",
    "    w5=Wbin*Waout\n",
    "    result.append(w4)\n",
    "    result.append(w5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0020502306509482316\n",
      "0.012161144797036215\n",
      "0.6224424629616742\n",
      "0.010785645054603582\n"
     ]
    }
   ],
   "source": [
    "print (Jaccard(\"1272125\",\"2023163\"))\n",
    "print (salton(\"1272125\",\"2023163\"))\n",
    "print (AA(\"1272125\",\"2023163\"))\n",
    "print (allocation(\"1272125\",\"2023163\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output result for different similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fp = open(\"test-public.txt\",\"r\")\n",
    "header = fp.readline()\n",
    "test_data = fp.readlines()\n",
    "fp.close()\n",
    "with open('result_Jaccard.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Id\",\"Prediction\"])\n",
    "    for line in test_data:\n",
    "        test_id,node1,node2 = line.split()\n",
    "        writer.writerow([test_id,Jaccard(node1,node2)])\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fp = open(\"test-public.txt\",\"r\")\n",
    "header = fp.readline()\n",
    "test_data = fp.readlines()\n",
    "fp.close()\n",
    "with open('result_salton.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Id\",\"Prediction\"])\n",
    "    for line in test_data:\n",
    "        test_id,node1,node2 = line.split()\n",
    "        writer.writerow([test_id,salton(node1,node2)])\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fp = open(\"test-public.txt\",\"r\")\n",
    "header = fp.readline()\n",
    "test_data = fp.readlines()\n",
    "fp.close()\n",
    "with open('result_AA.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Id\",\"Prediction\"])\n",
    "    for line in test_data:\n",
    "        test_id,node1,node2 = line.split()\n",
    "        writer.writerow([test_id,AA(node1,node2)])\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fp = open(\"test-public.txt\",\"r\")\n",
    "header = fp.readline()\n",
    "test_data = fp.readlines()\n",
    "fp.close()\n",
    "with open('result_allocation.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Id\",\"Prediction\"])\n",
    "    for line in test_data:\n",
    "        test_id,node1,node2 = line.split()\n",
    "        writer.writerow([test_id,allocation(node1,node2)])\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47893204\n"
     ]
    }
   ],
   "source": [
    "sum_deg = 0\n",
    "for node in node_set:\n",
    "    deg = len(in_neighbour[node])+len(out_neighbour[node])\n",
    "    sum_deg += deg\n",
    "print (sum_deg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNPA(node1,node2):\n",
    "    result = 0\n",
    "    sigma = 0.0000000001\n",
    "    PA = (len(in_neighbour[node1])+len(out_neighbour[node1]))*(len(in_neighbour[node2])+len(out_neighbour[node2]))\n",
    "    return common_neighbour(node1,node2)+(sigma*PA)/(sum_deg/len(node_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New approach with new data selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19570\n"
     ]
    }
   ],
   "source": [
    "sourse_set = set()\n",
    "sink_set = set()\n",
    "for node in out_neighbour.keys():\n",
    "    if len(out_neighbour[node])!=0:\n",
    "        sourse_set.add(node)\n",
    "print (len(sourse_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4847566\n"
     ]
    }
   ],
   "source": [
    "for node in in_neighbour.keys():\n",
    "    if len(in_neighbour.keys())!=0 and node not in sourse_set:\n",
    "        sink_set.add(node)\n",
    "print (len(sink_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "positive_list = set()\n",
    "source_list = random.sample(sourse_set,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204518\n",
      "149791\n"
     ]
    }
   ],
   "source": [
    "sink_list = set()\n",
    "for source in source_list:\n",
    "    for sink in out_neighbour[source]:\n",
    "        sink_list.add(sink)\n",
    "        positive_list.add(tuple([source,sink]))\n",
    "print (len(positive_list))\n",
    "print (len(sink_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37243232\n"
     ]
    }
   ],
   "source": [
    "negative_list = set()\n",
    "for source in source_list:\n",
    "    for sink in sink_list:\n",
    "        if tuple([source,sink]) not in positive_list:\n",
    "            negative_list.add(tuple([source,sink]))\n",
    "print (len(negative_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101094\n"
     ]
    }
   ],
   "source": [
    "negative_list2 = set()\n",
    "while len(negative_list2) < 100000:\n",
    "    for source in source_list:\n",
    "        unconnected = sink_list.difference(out_neighbour[source])\n",
    "        nodes = random.sample(unconnected,5)\n",
    "        for sink in nodes:\n",
    "            negative_list2.add(tuple([source,sink]))\n",
    "print (len(negative_list2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label the selected training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305612\n"
     ]
    }
   ],
   "source": [
    "training_set = []\n",
    "for node1,node2 in positive_list:\n",
    "    feature = []\n",
    "    feature.append(CNPA(node1,node2))\n",
    "    feature.append(salton(node1,node2))\n",
    "    #feature.append(AA(node1,node2))\n",
    "    feature.append(allocation(node1,node2))\n",
    "    feature.extend(knn(node1,node2))\n",
    "    feature.append(1)\n",
    "    training_set.append(feature)\n",
    "for node1,node2 in negative_list2:\n",
    "    feature = []\n",
    "    feature.append(CNPA(node1,node2))\n",
    "    feature.append(salton(node1,node2))\n",
    "    #feature.append(AA(node1,node2))\n",
    "    feature.append(allocation(node1,node2))\n",
    "    feature.extend(knn(node1,node2))\n",
    "    feature.append(0)\n",
    "    training_set.append(feature)\n",
    "print (len(training_set))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "for item in training_set:\n",
    "    X_train.append(item[:len(item)-1])\n",
    "    Y_train.append(item[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [50,100]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [20,50]\n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "para_grid = {'n_estimators': n_estimators,\n",
    "               #'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50 \n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50 \n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50 \n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50, total=   1.8s\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50, total=  54.5s\n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50, total=  54.9s\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100 \n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100, total=   2.3s\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100, total= 1.8min\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50, total=  58.0s\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100, total= 1.9min\n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50, total=   2.0s\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100 \n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50, total=  59.0s\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100, total=   2.9s\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50, total= 1.0min\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50, total= 1.0min\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50, total=   1.8s\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100, total= 2.0min\n",
      "[CV] max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100, total= 2.0min\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100, total=   3.1s\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50, total=  58.8s\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50, total=  58.1s\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50, total=   1.7s\n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100, total= 2.0min\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100 \n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100, total= 2.0min\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100, total=   2.3s\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50, total= 1.0min\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50, total=   1.8s\n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50, total= 1.0min\n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100, total= 1.9min\n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100, total= 1.9min\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100 \n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100 \n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100 \n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100, total=   2.5s\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50, total=  56.4s\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50, total=   1.6s\n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50, total=  56.3s\n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100, total= 1.9min\n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100, total= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  9.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100, total= 1.8min\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100, total= 2.1min\n",
      "[CV] max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100 \n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50 \n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50 \n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100, total=   3.9s\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50, total=   2.6s\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50, total= 1.3min\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50, total= 1.3min\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100, total=   2.6s\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50, total=  56.0s\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100, total= 2.2min\n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100, total= 2.2min\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100 \n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50, total=   1.7s\n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50, total=  55.8s\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100 \n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100, total=   2.3s\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50, total=  54.9s\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50, total=   1.6s\n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50, total=  54.6s\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100 \n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100, total= 1.8min\n",
      "[CV] max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100, total= 1.8min\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100, total=   2.4s\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100, total= 1.8min\n",
      "[CV]  max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100, total= 1.8min\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=50, total=  56.8s\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=50, total=   1.7s\n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=50, total=  56.5s\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100, total=   2.3s\n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=50, total= 1.4min\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=50 \n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=50, total=   2.7s\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100, total= 2.4min\n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100, total= 2.4min\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=100, total=   2.4s\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=50, total= 1.1min\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=50, total=  58.1s\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=50, total=   1.8s\n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=50, total= 1.1min\n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=100, total= 2.1min\n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=5, n_estimators=100, total= 2.1min\n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=100, total=   3.8s\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=100, total= 2.2min\n",
      "[CV]  max_depth=50, min_samples_leaf=1, min_samples_split=10, n_estimators=100, total= 2.2min\n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=50, total=  57.1s\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=50 \n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=50 \n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=50, total=   1.8s\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100, total=   2.4s\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=50, total= 1.0min\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=50, total= 1.0min\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=50, total=   1.7s\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=50, total= 1.0min\n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100, total= 2.0min\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100, total= 2.0min\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100, total=   3.1s\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=50, total= 1.0min\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=50, total=   1.8s\n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=50, total=  59.5s\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100, total= 2.1min\n",
      "[CV] max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=100, total=   2.4s\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100, total= 2.1min\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=50, total= 1.0min\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=50, total=   1.6s\n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=50, total= 1.1min\n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=100, total= 2.2min\n",
      "[CV]  max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=100, total= 2.2min\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=100, total=   2.8s\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=50, total= 1.0min\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=50, total= 1.0min\n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=50, total=   2.0s\n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=100, total= 2.0min\n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=100, total= 2.1min\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=100, total=   2.5s\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=50, total=  55.7s\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=50 \n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=50, total=   1.7s\n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=50, total=  55.9s\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=100 \n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=100, total= 1.8min\n",
      "[CV] max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=100 \n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=100, total= 1.9min\n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=100, total=   2.2s\n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=100, total= 1.5min\n",
      "[CV]  max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=100, total= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 41.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "train success!\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#rf = RandomForestRegressor(max_depth = 2)\n",
    "#rf.fit(X_train,Y_train)\n",
    "\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#rf = RandomForestRegressor(max_depth = 2)\n",
    "rf = RandomForestRegressor(bootstrap = True, max_depth = 20)\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = para_grid, cv = 3, verbose=2, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "grid_search.fit(X_train, Y_train)\n",
    "#rf.fit(X_train,Y_train)\n",
    "best_grid = grid_search.best_estimator_\n",
    "print (best_grid)\n",
    "print (\"train success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t2184483\t1300190\n",
      "\n",
      "2000\n",
      "[5.640174919180601e-09, 0.0, 0, 0.6091089451179962, 0.0545544725589981]\n"
     ]
    }
   ],
   "source": [
    "# predict test-public and write in file\n",
    "import csv\n",
    "fp = open(\"test-public.txt\",\"r\")\n",
    "head = fp.readline()\n",
    "test_file = fp.readlines()\n",
    "fp.close()\n",
    "print (test_file[0])\n",
    "test_data = []\n",
    "for line in test_file:\n",
    "    Id,node1,node2 = line.split()\n",
    "    feature = []\n",
    "    feature.append(CNPA(node1,node2))\n",
    "    feature.append(salton(node1,node2))\n",
    "    #feature.append(AA(node1,node2))\n",
    "    feature.append(allocation(node1,node2))\n",
    "    feature.extend(knn(node1,node2))\n",
    "    test_data.append(feature)\n",
    "print (len(test_data))\n",
    "print (test_data[0])\n",
    "with open(\"random forest new2.csv\",\"w\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Id\",\"Prediction\"])\n",
    "    for i in range(len(test_data)):\n",
    "        predict = grid_search.predict([test_data[i]])[0]\n",
    "        writer.writerow([i+1,predict])\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
